* add raw audio to the HDF5 file (maybe rename it to dataset.h5)

* measure if realtime spectrogram extraction during training is viable

* implement random equalization augmentation
* implement time stretching
* implement freq/pitch shifting
* implement non-integer time shifting

* implement L2 version of Crammer and Singer hinge loss
    * version with sum over labels
    * version with max over incorrect labels

* add refilter layers after convolution layers to reduce the number of parameters

* add more augmentation methods: non-integer shifts, small frequency shifts, time stretching (use scikit-image code from kaggle-galaxies)

* confusion matrices could be very informative, compute them


* loss can still be NaN for softmax + crossentropy, using the clipped crossentropy is a bit of a hack. Is there a cleaner way to solve this?
