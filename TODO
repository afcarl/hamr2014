* implement a new build_chunk function to do more advanced augmentation at the spectrogram level:
    * non-integer time shifting
    * non-integer freq shifting

    * time stretching

    * additive / multiplicative noise

    * random equalization augmentation

    - use scikit-image (look into how to do this again)
    - don't forget to apply compression afterwards



* consider doing the spectrogram extraction on the GPU, with data loaded onto the GPU per batch. This might be fast enough and allows for: 
    * changing window sizes / hop sizes
    * augmentation in the time domain
        - time stretching and frequency shifting should be easier in the frequency domain though (need a time-freq representation for this anyway)



* implement L2 version of Crammer and Singer hinge loss
    * version with sum over labels
    * version with max over incorrect labels

* add refilter layers after convolution layers to reduce the number of parameters

* add more augmentation methods: non-integer shifts, small frequency shifts, time stretching (use scikit-image code from kaggle-galaxies)

* confusion matrices could be very informative, compute them


* loss can still be NaN for softmax + crossentropy, using the clipped crossentropy is a bit of a hack. Is there a cleaner way to solve this?
